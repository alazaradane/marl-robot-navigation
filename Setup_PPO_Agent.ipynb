{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3rKEQO6/scgcJEV2cSkoe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alazaradane/marl-robot-navigation/blob/main/Setup_PPO_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup PPO Agent"
      ],
      "metadata": {
        "id": "Tns6PfQQxHvY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm-R6lHxxA0P"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.agents.ppo import ppo_agent\n",
        "from tf_agents.networks import actor_distribution_network, value_network\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils.common import function\n",
        "\n",
        "# Wrap environment for TF-Agents\n",
        "train_env = tf_py_environment.TFPyEnvironment(DroneEnvironment())\n",
        "eval_env = tf_py_environment.TFPyEnvironment(DroneEnvironment())\n",
        "\n",
        "# Define networks for PPO\n",
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=(64, 64)\n",
        ")\n",
        "value_net = value_network.ValueNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    fc_layer_params=(64, 64)\n",
        ")\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# Define PPO Agent\n",
        "agent = ppo_agent.PPOAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    optimizer=optimizer,\n",
        "    actor_net=actor_net,\n",
        "    value_net=value_net,\n",
        "    num_epochs=10,\n",
        "    train_step_counter=tf.Variable(0)\n",
        ")\n",
        "agent.initialize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Agent"
      ],
      "metadata": {
        "id": "SESYq33rxUSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a replay buffer, collect experiences, and train the agent:"
      ],
      "metadata": {
        "id": "vK7GIegNxg8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.policies import random_tf_policy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fO_yVdlixlw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from lib.plotters import Plotter\n",
        "from lib.customEnvironment_v0_8 import DroneEnvironment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import TimeLimit\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
        "from tf_agents.networks.value_network import ValueNetwork\n",
        "from tf_agents.agents import PPOAgent\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.utils import common\n",
        "\n",
        "np.random.seed()\n",
        "tf.random.set_seed()\n",
        "\n",
        "\n",
        "# Reinforcement Learning parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PPO is an on-policy algorithm, so maybe lower the initial data collection?\n",
        "\n",
        "# Replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=10000\n",
        ")\n",
        "\n",
        "# Random policy for data collection\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "    time_step = environment.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = environment.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    buffer.add_batch(traj)\n",
        "\n",
        "replay_buffer_capacity = 1000000\n",
        "initial_collect_steps = 1000 # total number of steps collected with a random policy. Every time the steps TimeLimit is reached, the environment is reset\n",
        "\n",
        "# Agent\n",
        "fc_layer_params = (64, 64,)\n",
        "\n",
        "# Training\n",
        "train_env_steps_limit = 200 # maximum number of steps in the TimeLimit of the training environment\n",
        "collect_steps_per_iteration = 200 # maximum number of steps in each episode\n",
        "\n",
        "epochs = 4000\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "checkpoint_dir = save_path + '/ckpts'\n",
        "policy_dir = save_path + '/policies'\n",
        "ckpts_interval = 10 # every how many epochs to store a checkpoint during training\n",
        "\n",
        "# Evaluation\n",
        "eval_env_steps_limit = 400 # maximum number of steps in the TimeLimit of the evaluation environment\n",
        "num_eval_episodes = 5\n",
        "eval_interval = 50 # interval for evaluation and policy saving, =epochs for evaluation only at the end\n",
        "\n",
        "\n",
        "# Environments instantiation\n",
        "\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(TimeLimit(DroneEnvironment(), duration=train_env_steps_limit)) # set limit to 100 steps in the environment\n",
        "eval_tf_env = tf_py_environment.TFPyEnvironment(TimeLimit(DroneEnvironment(), duration=eval_env_steps_limit)) # 1000 steps duration\n",
        "\n",
        "\n",
        "\n",
        "# Agent\n",
        "\n",
        "\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "actor_net = ActorDistributionNetwork(tf_env.observation_spec(), tf_env.action_spec(), fc_layer_params=fc_layer_params, activation_fn=tf.keras.activations.tanh)\n",
        "value_net = ValueNetwork(tf_env.observation_spec(), fc_layer_params=fc_layer_params, activation_fn=tf.keras.activations.tanh)\n",
        "\n",
        "agent = PPOAgent(tf_env.time_step_spec(),\n",
        "                 tf_env.action_spec(),\n",
        "                 actor_net=actor_net,\n",
        "                 value_net=value_net,\n",
        "                 optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                 discount_factor=0.99,\n",
        "                 num_epochs=1,\n",
        "                 train_step_counter=global_step)\n",
        "agent.initialize()\n",
        "\n",
        "print(\"\\nActor network summary and details\")\n",
        "print(actor_net.summary())\n",
        "for i, layer in enumerate (actor_net.layers):\n",
        "    print (i, layer)\n",
        "    try: print (\"    \",layer.activation)\n",
        "    except AttributeError: print('   no activation attribute')\n",
        "\n",
        "print(\"\\nCritic network summary and details\")\n",
        "print(value_net.summary())\n",
        "for i, layer in enumerate (value_net.layers):\n",
        "    print (i, layer)\n",
        "    try: print (\"    \",layer.activation)\n",
        "    except AttributeError: print('   no activation attribute')\n",
        "\n",
        "\n",
        "# Replay Buffer & Collect Driver\n",
        "\n",
        "\n",
        "# PPO is an on-policy algorithm, so it makes sense to collect only on-policy data\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec, batch_size=tf_env.batch_size, max_length=replay_buffer_capacity)\n",
        "\n",
        "# Create the collect driver\n",
        "num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "observers = [replay_buffer.add_batch, num_episodes, env_steps]\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(tf_env, agent.collect_policy, observers=observers, num_steps=initial_collect_steps) # use tf_policy, which is random\n",
        "\n",
        "train_driver = dynamic_step_driver.DynamicStepDriver(tf_env, agent.collect_policy, observers=observers, num_steps=collect_steps_per_iteration) # instead of tf_policy use the agent.collect_policy, which is the OUNoisePolicy\n",
        "\n",
        "# Initial data collection\n",
        "print('\\nCollecting initial data')\n",
        "collect_driver.run()\n",
        "print('Data collection executed\\n')\n",
        "\n",
        "# Transform Replay Buffer to Dataset\n",
        "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3) # read batches of 32 elements, each with 2 timesteps\n",
        "iterator = iter(dataset)\n",
        "\n",
        "\n",
        "\n",
        "# Training and Evaluation functions\n",
        "\n",
        "\n",
        "train_checkpointer = common.Checkpointer(ckpt_dir=checkpoint_dir, max_to_keep=1, agent=agent, policy=agent.policy, replay_buffer=replay_buffer, global_step=global_step)\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "\n",
        "data_plotter = Plotter()\n",
        "\n",
        "def train_one_iteration():\n",
        "  start = time.time()\n",
        "  train_driver.run() # collect a few steps using collect_policy and save to the replay buffer\n",
        "  end = time.time()\n",
        "  experience, unused_info = next(iterator) # sample a batch of data from the buffer and update the agent's network\n",
        "  with tf.device('/CPU:0'): train_loss = agent.train(experience) # trains on 1 batch of experience\n",
        "  iteration = agent.train_step_counter.numpy()\n",
        "  #data_plotter.update_loss(train_loss.loss)\n",
        "  print ('Iteration:', iteration)\n",
        "  print('Total_loss:', float(train_loss.loss), 'actor_loss:', float(train_loss.extra.actor_loss), 'critic_loss:', float(train_loss.extra.critic_loss))\n",
        "  print('Control loop timing for 1 timestep [s]:', (end-start)/collect_steps_per_iteration)\n",
        "\n",
        "def evaluate_agent(policy, eval_tf_env, num_eval_episodes):\n",
        "  print('\\nEVALUATING *******\\n')\n",
        "  total_reward = 0\n",
        "  for idx in range(num_eval_episodes):\n",
        "    print('Evaluation iteration:', idx)\n",
        "    start = time.time()\n",
        "    time_step = eval_tf_env.reset()\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = eval_tf_env.step(action_step.action)\n",
        "      total_reward += float(time_step.reward)\n",
        "    end = time.time()\n",
        "    print('Control loop timing for 1 timestep [s]:', (end-start)/eval_env_steps_limit)\n",
        "  print('\\n******* EVALUATION ENDED\\n')\n",
        "  return total_reward / num_eval_episodes # avg reward per episode\n",
        "\n",
        "# Training loop, evaluation & checkpoints saving\n",
        "avg_rewards = np.empty((0,2))\n",
        "for epoch in range(epochs+1):\n",
        "  train_one_iteration()\n",
        "  if epoch % ckpts_interval == 0:\n",
        "    train_checkpointer.save(global_step)\n",
        "  if epoch % eval_interval == 0:\n",
        "    tf_policy_saver.save(policy_dir) # policy saving for later restore\n",
        "    avg_rew = evaluate_agent(agent.policy, eval_tf_env, num_eval_episodes)\n",
        "    avg_rewards = np.concatenate((avg_rewards, [[epoch, avg_rew]]), axis=0)\n",
        "    data_plotter.update_eval_reward(avg_rew, eval_interval)\n",
        "\n",
        "np.save(save_path+'/avg_rewards.npy', avg_rewards)\n",
        "\n",
        "data_plotter.plot_evaluation_rewards(avg_rewards, save_path)\n",
        "num_iterations = 10000\n",
        "for _ in range(num_iterations):\n",
        "    collect_step(train_env, random_policy, replay_buffer)\n",
        "    experience = replay_buffer.gather_all()\n",
        "    train_loss = agent.train(experience)\n",
        "    replay_buffer.clear()\n",
        "\n",
        "    if _ % 100 == 0:\n",
        "        print(f\"Iteration {_}: Loss = {train_loss.loss.numpy()}\")\n",
        "\n",
        "\n",
        "# Restoring only the policy\n",
        "#saved_policy = tf.saved_model.load(policy_dir)"
      ],
      "metadata": {
        "id": "ZcSCE4IZ4UEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the Policy"
      ],
      "metadata": {
        "id": "cDnIdbwjxpQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the learned policy in the environment:"
      ],
      "metadata": {
        "id": "nMhITQvNxtmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, environment, num_episodes=10):\n",
        "    total_rewards = []\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_reward = 0\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step)\n",
        "            time_step = environment.step(action_step.action)\n",
        "            episode_reward += time_step.reward.numpy()\n",
        "        total_rewards.append(episode_reward)\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    print(f\"Average Reward: {avg_reward}\")\n",
        "\n",
        "evaluate_policy(agent.policy, eval_env)\n"
      ],
      "metadata": {
        "id": "ooISduAYx1KZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}