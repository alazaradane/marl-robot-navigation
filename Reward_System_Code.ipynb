{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2958JObcHUPJH+BHPMxf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alazaradane/marl-robot-navigation/blob/main/Reward_System_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reward System Code"
      ],
      "metadata": {
        "id": "dwHshel7v6jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explanation of the Reward Function:\n"
      ],
      "metadata": {
        "id": "6-eTK6QoxUUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target Progress:\n",
        "\n",
        "Dist to Target: The reward becomes positive when the drone reaches the target point, with a large reward of 50 for reaching it. The drone receives a negative reward for moving away from the target.\n",
        "Start Point Penalty:\n",
        "\n",
        "Dist to Start: A small penalty is added for moving too far away from the start point, encouraging the drone to take the shortest path toward the target.\n",
        "Collision Penalty:\n",
        "\n",
        "A penalty (collision_penalty = -10.0) is applied if the drone collides with an obstacle, encouraging safe navigation."
      ],
      "metadata": {
        "id": "vgFW7CXaxhDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orange Plot Parameters:\n",
        "\n",
        "orange_center: This is the center of the danger zone (the orange plot).\n",
        "\n",
        "orange_radius: This defines the radius of the circular danger zone. You can adjust this value based on the size of the zone you want to create.\n",
        "\n",
        "Penalty for Entering the Orange Plot:\n",
        "\n",
        "Full Penalty: If the drone enters the orange plot (i.e., the distance to the center of the plot is less than or equal to orange_radius), a large penalty of -50 is applied.\n",
        "\n",
        "Proximity Penalty: If the drone is near the orange plot but outside of it (i.e., the distance is between orange_radius and 1.5 * orange_radius), a smaller penalty of -20 is applied.\n",
        "\n",
        "Smooth Movement Encouraged: The drone is still encouraged to move smoothly and efficiently with penalties for excessive movement or unnecessary steps.\n",
        "\n",
        "Time Efficiency:\n",
        "\n",
        "A small penalty (time_penalty = -0.1) is applied for each step, encouraging the drone to reach the target in the least amount of time."
      ],
      "metadata": {
        "id": "7kvAgiNbxmYm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSxAxzVVxPZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CustomDroneRewardCalculator:\n",
        "    def __init__(self, start_point, target_point, orange_center, orange_radius, threshold=0.5, collision_penalty=-10.0, time_penalty=-0.1):\n",
        "        # Starting point and target point\n",
        "        self.start_point = np.array(start_point)\n",
        "        self.target_point = np.array(target_point)\n",
        "        # Orange plot (danger zone)\n",
        "        self.orange_center = np.array(orange_center)\n",
        "        self.orange_radius = orange_radius\n",
        "        # Threshold for considering the target \"reached\"\n",
        "        self.threshold = threshold\n",
        "        # Penalty for collision\n",
        "        self.collision_penalty = collision_penalty\n",
        "        # Penalty for inefficiency in time taken\n",
        "        self.time_penalty = time_penalty\n",
        "\n",
        "        # State tracking\n",
        "        self.prev_position = None\n",
        "        self.total_steps = 0\n",
        "        self.done = False\n",
        "\n",
        "    def update_position(self, current_position):\n",
        "        \"\"\"\n",
        "        Update the drone's position.\n",
        "        \"\"\"\n",
        "        self.prev_position = np.copy(current_position)\n",
        "\n",
        "    def calculate_reward(self, current_position, action, collision_occurred):\n",
        "        \"\"\"\n",
        "        Calculate the reward based on the drone's current position, action, and whether a collision occurred.\n",
        "        \"\"\"\n",
        "        # 1. Progress toward the target point\n",
        "        dist_to_target = np.linalg.norm(current_position - self.target_point)\n",
        "        dist_to_start = np.linalg.norm(current_position - self.start_point)\n",
        "\n",
        "        # Reward for moving toward the target point\n",
        "        if dist_to_target <= self.threshold:\n",
        "            reward = 50  # Large reward for reaching the target\n",
        "            self.done = True\n",
        "        else:\n",
        "            reward = -dist_to_target  # Negative reward based on distance to target\n",
        "\n",
        "        # Reward for moving away from the start point (penalizing inefficiency)\n",
        "        reward -= dist_to_start * 0.1\n",
        "\n",
        "        # 2. Collision penalty (if any)\n",
        "        if collision_occurred:\n",
        "            reward += self.collision_penalty\n",
        "\n",
        "        # 3. Efficiency: penalize for excessive actions (to encourage smooth flight)\n",
        "        if self.prev_position is not None:\n",
        "            movement_penalty = np.linalg.norm(current_position - self.prev_position)\n",
        "            reward -= movement_penalty  # Reward decreases as movement becomes erratic or excessive\n",
        "\n",
        "        # 4. Time penalty (to minimize time spent)\n",
        "        self.total_steps += 1\n",
        "        reward += self.time_penalty  # Penalize each step to encourage faster completion\n",
        "\n",
        "        # 5. Avoiding the Orange Plot (Danger Zone)\n",
        "        # Calculate distance from the drone's position to the center of the orange plot\n",
        "        dist_to_orange_plot = np.linalg.norm(current_position - self.orange_center)\n",
        "\n",
        "        # Apply a penalty if the drone is inside or too close to the orange plot\n",
        "        if dist_to_orange_plot <= self.orange_radius:\n",
        "            reward -= 50  # Large penalty for entering the danger zone (orange plot)\n",
        "        elif dist_to_orange_plot <= self.orange_radius * 1.5:\n",
        "            reward -= 20  # Smaller penalty if drone is near but not inside the danger zone\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset tracking variables at the start of an episode.\n",
        "        \"\"\"\n",
        "        self.prev_position = None\n",
        "        self.total_steps = 0\n",
        "        self.done = False\n"
      ],
      "metadata": {
        "id": "hdDFO71Aw5dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}